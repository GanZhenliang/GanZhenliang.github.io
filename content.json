{"meta":{"title":"Zhenliang'Blog","subtitle":"","description":"","author":"Zhenliang_Gan","url":"https://www.zlgan.top","root":"/"},"pages":[{"title":"","date":"2022-10-05T13:45:01.037Z","updated":"2022-10-05T13:45:01.037Z","comments":true,"path":"about/index.html","permalink":"https://www.zlgan.top/about/index.html","excerpt":"","text":"躺平的佛系码猿~"},{"title":"My friends","date":"2022-03-25T17:08:20.207Z","updated":"2022-03-25T17:08:20.207Z","comments":true,"path":"friends/index.html","permalink":"https://www.zlgan.top/friends/index.html","excerpt":"欢迎我的大佬朋友们和我互加友链！【友链信息见文章末尾。","text":"欢迎我的大佬朋友们和我互加友链！【友链信息见文章末尾。 12345678910- title: # 网站名称---必 url: # 访问地址---必 avatar: # 头像地址---必 description: # 描述/一句话概述/格言---选 screenshot: # 网站截图/展示图---选 backgroundColor: # 头像背景颜色---选 textColor: # 文本颜色---选 keywords:---选 - 标签一 - 标签二"},{"title":"所有分类","date":"2022-03-25T04:13:10.125Z","updated":"2022-03-25T04:13:10.125Z","comments":true,"path":"categories/index.html","permalink":"https://www.zlgan.top/categories/index.html","excerpt":"","text":""},{"title":"所有标签","date":"2022-03-25T04:13:42.872Z","updated":"2022-03-25T04:13:42.872Z","comments":true,"path":"tags/index.html","permalink":"https://www.zlgan.top/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"持续学习","slug":"终身学习","date":"2022-11-22T04:34:05.856Z","updated":"2022-11-22T04:31:53.456Z","comments":true,"path":"2022/11/22/终身学习/","link":"","permalink":"https://www.zlgan.top/2022/11/22/%E7%BB%88%E8%BA%AB%E5%AD%A6%E4%B9%A0/","excerpt":"","text":"持续学习半监督 利用labeled data训练一个模型 利用训练好的模型，对unlabled data进行predict，得到标注，称为pseudo label 从pseudo label中选取一些样本，放入labelled data中。选取方法自定义，比如放入置信度比较高 重新训练模型，然后继续迭代。 life long learning 先学A再学B会遗忘之前学到的A 两个同时扔进去学习可以解决准确率问题，但是海量数据同时存储（前999个任务的数据+当前任务数据）不现实，存储量大，计算量大。 解决方法1 EWC（Elastic Weight Consolidation） bi为参数更新权重，表示参数θi有多重要，能不能变化太多 核心思路：保持对原来任务很重要的参数，只改变对原来任务影响不大的参数 bi怎么得到EWC利用二阶导数来确定bi。如果二阶导数比较小，位于一个平坦的盆地中，则表明参数变化对task影响小，此时bi小。如果二阶导数比较大，位于一个狭小的山谷中，则表明参数变化对task影响大，此时bi大。 解决方法2 生成样本在学习task2时，我们可以利用生成模型，生成task1的样本。然后利用task1和task2的样本，进行多任务学习，使得模型在两个task上均表现很好。 难点在于，生成样本，特别是复杂的样本，目前难度是很大的。 解决方法3 Knowledge Transfer 知识迁移迁移学习只关注迁移后模型在target task上要表现好，至于source task则不关心。 终生学习则需要模型在两个task上都表现好，不能出现遗忘。 将终生学习理解为更严格的迁移学习。 解决方法4 Model Expansion 模型扩张方法一 progressive neural network 将task1每层输出的feature，作为task2对应层的输入 方法二 Net2Net对网络进行加宽 由神经元h2分裂得到一个神经元h3 h2和h3权重均为之前h2权重的一半 在h3上添加一些小噪声，使二者能够独立更新。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://www.zlgan.top/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://www.zlgan.top/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"终生学习","slug":"终生学习","permalink":"https://www.zlgan.top/tags/%E7%BB%88%E7%94%9F%E5%AD%A6%E4%B9%A0/"}],"author":"gzl"},{"title":"国内现有的BM资质或标准（三大类：军工BM资质、SM集成资质和机关单位自查自评工作守则）","slug":"国内现有的BM资质或标准（三大类：军工BM资质、SM集成资质和机关单位自查自评工作守则）","date":"2022-04-26T03:47:05.355Z","updated":"2022-04-26T03:52:05.538Z","comments":true,"path":"2022/04/26/国内现有的BM资质或标准（三大类：军工BM资质、SM集成资质和机关单位自查自评工作守则）/","link":"","permalink":"https://www.zlgan.top/2022/04/26/%E5%9B%BD%E5%86%85%E7%8E%B0%E6%9C%89%E7%9A%84BM%E8%B5%84%E8%B4%A8%E6%88%96%E6%A0%87%E5%87%86%EF%BC%88%E4%B8%89%E5%A4%A7%E7%B1%BB%EF%BC%9A%E5%86%9B%E5%B7%A5BM%E8%B5%84%E8%B4%A8%E3%80%81SM%E9%9B%86%E6%88%90%E8%B5%84%E8%B4%A8%E5%92%8C%E6%9C%BA%E5%85%B3%E5%8D%95%E4%BD%8D%E8%87%AA%E6%9F%A5%E8%87%AA%E8%AF%84%E5%B7%A5%E4%BD%9C%E5%AE%88%E5%88%99%EF%BC%89/","excerpt":"","text":"国内现有的BM资质或标准（三大类：军工BM资质、SM集成资质和机关单位自查自评工作守则）一、相关概念保密资质全称为武器装备科研生产单位保密资格。保密资格认证是国家对武器装备科研生产单位的保密基本条件的评价和认可，是企事业单位承担涉密武器装备科研生产任务的必要条件。 涉密资质，全称为涉密系统集成资质。该资质是从事涉及国家秘密的计算机信息系统集成业务的单位需要取得的。 机关单位自查自评工作守则，是机关单位(中央和地方国家机关单位)根据相关要求，定期进行自查自省查缺补漏。 《国家秘密载体印制资质管理办法》《涉密信息系统集成资质管理办法》《武器装备科研生产单位保密资格审查认定办法》等现行的规章办法是保密资质管理工作中重要的标准。 二、具体分级军工保密资质军工保密资质，以前分为一级、二级、三级。自2021年7月1日起，武器装备科研生产单位保密资格，由三级调整为两级，取消三级资格$^{[1]}$。 为军工集团公司及所属承担涉密武器装备科研生产任务单位、地方军工单位委托法人单位和其他组织，提供审计、法律、证券、评估、招投标、翻译、设计、施工、监理、评价、物流（运输）、设备设施维修（检测）、展览展示等可直接涉及武器装备科研生产国家秘密的咨询服务活动需要进行军工涉密业务咨询服务安全保密监督审查，这个审查是谁委托谁监管的原则，审查主体是军工集团或者是涉密的甲方单位，最终的监管部门是国家国防科技工业局，具体审查管理办法也是国家国防工业局来制定的。 此项目不发证书。 涉密信息系统集成资质涉密信息系统集成资质，分为甲乙两级，甲级可以在全国范围内从事最高绝密级信息系统集成业务，乙级单位可以在注册地省、自治区、直辖市行政区域内从事最高机密级信息系统集成业务。这个资质的要求要比军密更高一些，比如对企业的专业人员数量、业绩数量、保密场所大小等，不符合的无法完成。此证书的发证单位为国家保密局。分为九个单项（集成、咨询、软件开发、综合布线、安防监控、屏蔽性建设、运行维护、数据恢复、工程监理）。 企业(2个印刷资质）国家秘密载体印制资质，国家秘密载体，是指以文字、数据、符号、图形、图像、声音等方式记载国家秘密信息的纸介质、光介质、电磁介质等各类物品。国家机关和涉及国家秘密的单位委托印制国家秘密载体，应当选择具有国家秘密载体印制资质的单位，国家秘密载体印制资质实行分类分级管理。 甲级资质单位可以在全国范围内承担最高绝密级印制业务，乙级在所在地最高机密级印制业务。 乙级跨省承接印制业务的，应当经委托业务的机关、单位所在地省、自治区、直辖市保密行政管理部门批准。此证由国家保密局颁发。 机关单位自查自评16项： 1、保密工作领导责任制 2、保密制度建设 3、保密宣传教育培训 4、涉密人员管理 5、国家秘密确定变更和解除 6、国家秘密载体管理 7、涉密场所和保密要害部门部位管理 8、信息系统和信息设备管理 9、宣传报道和信息公开管理 10、涉密会议和活动，货物、工程、服务采购管理 11、涉外工作保密管理 12、保密检查 13、违反保密法律法规行为查处情况 14、保密工作考核 15、保密组织机构设置、人员配备及经费保障 16、保密工作记录 三、主管部门涉密资质根据资质等级的不同，主管部门不同。申请甲级资质的，应当向国家保密行政管理部门提交申请书及相关材料；申请乙级资质的，应当向注册地的省、自治区、直辖市保密行政管理部门提交申请书及相关材料。 保密资质根据资质等级的不同，主管部门也不同。申请一级和二级保密资质，主管部门为国家保密局会同国家国防科技工业局、中央军委装备发展部等部门组成的国家武器装备科研生产单位保密资格认定委员会。 国务院于2021年6月2日印发《关于深化“证照分离”改革进一步激发市场主体发展活力的通知》$^{[1]}$，取消省级国防科技工业部门实施的“第二类武器装备科研生产许可（初审）”，申请人直接向国家国防科工局提出申请。 参考文献[1].重磅！军工资质做出重大调整！自2021年7月1日起，取消三级保密资质[J].中国军转民,2021(11):8. [2]罗中铭.保密资质管理工作中撤回、撤销、吊销、注销的概念辨析[J].保密工作,2021(06):57-59.DOI:10.19407&#x2F;j.cnki.cn11-2785&#x2F;d.2021.06.022.","categories":[{"name":"保密技术概论","slug":"保密技术概论","permalink":"https://www.zlgan.top/categories/%E4%BF%9D%E5%AF%86%E6%8A%80%E6%9C%AF%E6%A6%82%E8%AE%BA/"}],"tags":[{"name":"BM资质","slug":"BM资质","permalink":"https://www.zlgan.top/tags/BM%E8%B5%84%E8%B4%A8/"},{"name":"SM集成资质","slug":"SM集成资质","permalink":"https://www.zlgan.top/tags/SM%E9%9B%86%E6%88%90%E8%B5%84%E8%B4%A8/"},{"name":"保密","slug":"保密","permalink":"https://www.zlgan.top/tags/%E4%BF%9D%E5%AF%86/"}],"author":"gzl"},{"title":"ANN-Based Anomaly Detection","slug":"ANN-Based-Anomaly-Detection","date":"2022-04-23T11:56:56.807Z","updated":"2022-04-23T12:10:28.292Z","comments":true,"path":"2022/04/23/ANN-Based-Anomaly-Detection/","link":"","permalink":"https://www.zlgan.top/2022/04/23/ANN-Based-Anomaly-Detection/","excerpt":"","text":"ANN-Based Anomaly Detection数据集：CIC-IDS2017 数据条数：250w 左右 数据特征：78 数据标签：14 攻击 + 1正常 使用三个线性层（Linear） Project文件结构 Dataloder.py和CNN_Pytorch的Dataloder.py一样 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129import numpy as npimport pandas as pdfrom sklearn.model_selection import train_test_splitfrom sklearn import preprocessingimport time# 根据file读取数据def readData(file): print(&quot;Loading raw data...&quot;) raw_data = pd.read_csv(file, header=None, low_memory=False) return raw_data# 返回需要清除的行数listdef clearDirtyData(df): dropList = df[(df[14] == &quot;Nan&quot;) | (df[15] == &quot;Infinity&quot;)].index.tolist() return dropListdef lookData(raw_data): # 打印数据集的标签数据数量 # last_column_index = raw_data.shape[1] - 1 # print(raw_data[last_column_index].value_counts()) # 取出数据集标签部分 labels = raw_data.iloc[:, raw_data.shape[1] - 1:] # 多维数组转为以为数组 labels = labels.values.ravel() label_set = set(labels) return label_setdef separateData(raw_data): lists = raw_data.values.tolist() temp_lists = [] for i in range(0, 2): temp_lists.append([]) # 得到raw_data的数据标签集合 label_set = lookData(raw_data) # 将无序的数据标签集合转换为有序的list label_list = list(label_set) for i in range(0, len(lists)): # 得到所属标签的索引号 data_index = label_list.index(lists[i][len(lists[0]) - 1]) temp_lists[data_index].append(lists[i]) return temp_listsdef DataLoder(): acc_time = 0 start = time.time() data_1 = readData( &quot;/data/GanZhenliang/PycharmProjects/Anomaly_Detection/data/MachineLearningCVE/Monday-WorkingHours.pcap_ISCX.csv&quot;) data_1 = data_1.drop([0]) data_2 = readData( &quot;/data/GanZhenliang/PycharmProjects/Anomaly_Detection/data/MachineLearningCVE/Tuesday-WorkingHours.pcap_ISCX.csv&quot;) data_2 = data_2.drop([0]) data_3 = readData( &quot;/data/GanZhenliang/PycharmProjects/Anomaly_Detection/data/MachineLearningCVE/Wednesday-workingHours.pcap_ISCX.csv&quot;) data_3 = data_3.drop([0]) data_4 = readData( &quot;/data/GanZhenliang/PycharmProjects/Anomaly_Detection/data/MachineLearningCVE/Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv&quot;) data_4 = data_4.drop([0]) data_5 = readData( &quot;/data/GanZhenliang/PycharmProjects/Anomaly_Detection/data/MachineLearningCVE/Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv&quot;) data_5 = data_5.drop([0]) data_6 = readData( &quot;/data/GanZhenliang/PycharmProjects/Anomaly_Detection/data/MachineLearningCVE/Friday-WorkingHours-Morning.pcap_ISCX.csv&quot;) data_6 = data_6.drop([0]) data_7 = readData( &quot;/data/GanZhenliang/PycharmProjects/Anomaly_Detection/data/MachineLearningCVE/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv&quot;) data_7 = data_7.drop([0]) data_8 = readData( &quot;/data/GanZhenliang/PycharmProjects/Anomaly_Detection/data/MachineLearningCVE/Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv&quot;) data_8 = data_8.drop([0]) end = time.time() print(&#x27;数据加载时间：&#x27;, end - start, &#x27;s&#x27;) acc_time += end - start # print(&#x27;开始处理数据&#x27;) start = time.time() temp = [data_1, data_2, data_3, data_4, data_5, data_6, data_7, data_8] # temp = [data_7, data_8] data_all = pd.concat(temp) data_list_clear = clearDirtyData(data_all) data_all = data_all.drop(data_list_clear) # print(data_all[data_all.shape[1] - 1].value_counts()) data_all = data_all.sample(frac=1) print(data_all[data_all.shape[1] - 1].value_counts()) data_all[data_all.shape[1] - 1], attacks = pd.factorize(data_all[data_all.shape[1] - 1], sort=True) # data_all[78][data_all[78] != 0] = 1 print(data_all[data_all.shape[1] - 1].value_counts()) # ----------------------------------------降采样正常样本--------------------------------------- # data_all[78][data_all[78] != 0] = 1 # lists = separateData(data_all) # normal = pd.DataFrame(lists[0]) # normal = normal.sample(frac=0.5) # abnormal = pd.DataFrame(lists[1]) # temp = [normal, abnormal] # data_all = pd.concat(temp) # # print(data_all[data_all.shape[1] - 1].value_counts()) # ----------------------------------------数据处理，标准化、归一化、等--------------------------------------- features = data_all.iloc[:, :data_all.shape[1] - 1] labels = data_all.iloc[:, data_all.shape[1] - 1:] # 标准化.高斯分布(0,1) # features = preprocessing.scale(features) # 最小最大值标准化,最小0，最大1 features = preprocessing.minmax_scale(features, feature_range=(-3, 3)) # 归一化 # features = preprocessing.normalize(features, norm=&quot;l2&quot;) labels = labels.values.ravel() # labels[labels != 0] = 1 train_x, test_x, train_y, test_y = train_test_split(features, labels, test_size=0.3) # train_x, test_x, train_y, test_y = train_test_split(features, Y_onehot, test_size=0.3) end = time.time() print(&#x27;数据处理时间：&#x27;, end - start, &#x27;s&#x27;) acc_time += end - start return train_x, test_x, train_y, test_y, acc_time ANN.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160import pandas as pdimport torchimport torchvisionimport torch.nn as nnimport numpy as npimport matplotlib.pyplot as pltimport torch.utils.data as Datafrom Dataloder import DataLoderfrom sklearn import metricsimport seaborn as snsfrom sklearn.metrics import confusion_matrixepochs = 100batch_size = 128lr = 0.001train_data, test_data, train_label, test_label, acc_time = DataLoder()# 制作pytorch识别的数据集和定义模型train_data, train_label = torch.Tensor(train_data), torch.Tensor(train_label.T).long()test_data, test_label = torch.Tensor(test_data), torch.Tensor(test_label.T).long()train_dataset = Data.TensorDataset(train_data, train_label)test_dataset = Data.TensorDataset(test_data, test_label)# 制作Dataloder数据集，可迭代train_loader = Data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)test_loader = Data.DataLoader(test_dataset, batch_size=batch_size)# 如果是用gpu，就用gpu训练device = torch.device(&#x27;cuda:2&#x27; if torch.cuda.is_available() else &#x27;cpu&#x27;)# 定义模型num_inputs, num_hiddens, num_outputs = 78, 256, 15net = nn.Sequential( nn.Linear(num_inputs, num_hiddens), nn.ReLU(), nn.Linear(num_hiddens, 2 * num_hiddens), nn.ReLU(), nn.Linear(2 * num_hiddens, num_outputs)).to(device)# 定义损失函数loss = torch.nn.CrossEntropyLoss()# 定义优化器optimizer = torch.optim.Adam(net.parameters(), lr=lr)# 绘图def pltfigure(x, y, title, id, data): plt.subplot(2, 2, id) plt.plot(range(len(data)), data) plt.xlabel(x) plt.ylabel(y) plt.title(title, fontsize=10) # plt.show()# 模型的训练损失（loss）和验证损失（val_loss），以及训练准确率（acc）和验证准确率（val_acc）可以使用绘图代码绘制出来def visualize(loss, val_loss, acc, val_acc): epochs = range(1, len(loss) + 1) plt.plot(epochs, loss, &#x27;bo&#x27;, label=&#x27;Training loss&#x27;) plt.plot(epochs, val_loss, &#x27;b&#x27;, label=&#x27;Validation loss&#x27;) plt.title(&#x27;Training and validatio loss&#x27;) plt.xlabel(&#x27;Epochs&#x27;) plt.ylabel(&#x27;Loss&#x27;) plt.legend() plt.savefig(&#x27;result_ANN_Pytorch_loss.png&#x27;, bbox_inches=&#x27;tight&#x27;) plt.show() plt.figure() plt.plot(epochs, acc, &#x27;bo&#x27;, label=&#x27;Training acc&#x27;) plt.plot(epochs, val_acc, &#x27;b&#x27;, label=&#x27;Validation acc&#x27;) plt.title(&#x27;Training and validatio accuracy&#x27;) plt.legend() plt.savefig(&#x27;result_ANN_Pytorch_acc.png&#x27;, bbox_inches=&#x27;tight&#x27;) plt.show()def Confusion_matrix_hp(cm): cm = pd.DataFrame(cm) sns.heatmap(cm, annot=False, cmap=&#x27;GnBu&#x27;) plt.xlabel(&#x27;Real Label&#x27;) plt.ylabel(&#x27;Predict Label&#x27;) plt.savefig(&#x27;Confusion_ANN_Pytorch.png&#x27;, bbox_inches=&#x27;tight&#x27;) plt.show()# 训练def train(): net.train() batch_loss, correct, total = 0.0, 0.0, 0.0 for data, label in train_loader: data, label = data.to(device), label.to(device) net.zero_grad() output = net(data) l = loss(output, label) l.backward() optimizer.step() predict_label = torch.argmax(output, dim=1) correct += torch.sum(predict_label == label).cpu().item() total += len(label) batch_loss += l.cpu().item() return correct / total, batch_loss / len(train_loader)# 测试def test(): net.eval() batch_loss, correct, total = 0.0, 0.0, 0.0 predict = np.zeros((1, 1)) for data, label in test_loader: data, label = data.to(device), label.to(device) output = net(data) batch_loss += loss(output, label).cpu().item() predict_label = torch.argmax(output, dim=1) predict = np.append(predict, predict_label.data.cpu().numpy()) correct += torch.sum(predict_label == label).cpu().item() total += len(label) return correct / total, batch_loss / len(test_loader), predict[1:]# 主程序if __name__ == &quot;__main__&quot;: print(&#x27;training on: &#x27;, device) print(&#x27;batch_size:&#x27;, batch_size) print(&#x27;epochs:&#x27;, epochs) print(&#x27;learning_rate:&#x27;, lr) plt.figure() train_acc_list, train_loss_list, val_acc_list, val_loss_list = [], [], [], [] for epoch in range(epochs): train_acc, train_loss = train() test_acc, test_loss, predict = test() print(&#x27;epoch %d: train acc: %.2f%% train loss:%.4f, test acc: %.2f%%, test loss:%.4f&#x27; % (epoch, 100 * train_acc, train_loss, 100 * test_acc, test_loss)) train_acc_list.append(train_acc) train_loss_list.append(train_loss) val_acc_list.append(test_acc) val_loss_list.append(test_loss) test_y = np.array(test_label) print(metrics.classification_report(test_y, predict)) # 最后一个epoch的报告 cm = confusion_matrix(test_y, predict) Confusion_matrix_hp(cm) # 绘图 pltfigure(x=&#x27;epoch&#x27;, y=&#x27;acc&#x27;, title=&#x27;epoch-train_acc&#x27;, id=1, data=train_acc_list) pltfigure(x=&#x27;epoch&#x27;, y=&#x27;loss&#x27;, title=&#x27;epoch-train_loss&#x27;, id=2, data=train_loss_list) pltfigure(x=&#x27;epoch&#x27;, y=&#x27;acc&#x27;, title=&#x27;epoch-test_acc&#x27;, id=3, data=val_acc_list) pltfigure(x=&#x27;epoch&#x27;, y=&#x27;loss&#x27;, title=&#x27;epoch-test_loss&#x27;, id=4, data=val_loss_list) plt.tight_layout() plt.savefig(&#x27;result_ANN_Pytorch.png&#x27;, bbox_inches=&#x27;tight&#x27;) plt.show() visualize(train_loss_list, val_loss_list, train_acc_list, val_acc_list) 结果：123456epochs = 100batch_size = 128lr = 0.001百分之一数据集训练集 70%验证集 30% 15分类正常样本太多，各种攻击样本数量也非常不平衡，混淆矩阵没有任何意义 准确率，loss曲线 最后一个Epoch分类结果 precision recall f1-score support BENIGN 0.99 0.99 0.99 678355 Bot 0.58 0.60 0.59 566 DDoS 1.00 0.98 0.99 38051 DoS GoldenEye 0.99 0.96 0.97 3081 DoS Hulk 0.99 0.91 0.95 68930 DoS Slowhttptest 0.88 0.98 0.93 1654 DoS slowloris 0.99 0.96 0.97 1727 FTP-Patator 0.99 0.96 0.97 2305 Heartbleed 0.75 1.00 0.86 3 Infiltration 0.50 0.11 0.18 9 PortScan 0.85 0.99 0.91 46994 SSH-Patator 0.99 0.57 0.73 1808 Web Attack Brute Force 1.00 0.06 0.11 449 Web Attack Sql Injection 0.00 0.00 0.00 9 Web Attack XSS 0.00 0.00 0.00 199 accuracy 0.98 844140 macro avg 0.77 0.67 0.68 844140 weighted avg 0.98 0.98 0.98 844140","categories":[{"name":"入侵检测","slug":"入侵检测","permalink":"https://www.zlgan.top/categories/%E5%85%A5%E4%BE%B5%E6%A3%80%E6%B5%8B/"}],"tags":[{"name":"Anomaly Detection","slug":"Anomaly-Detection","permalink":"https://www.zlgan.top/tags/Anomaly-Detection/"},{"name":"ANN","slug":"ANN","permalink":"https://www.zlgan.top/tags/ANN/"},{"name":"机器学习","slug":"机器学习","permalink":"https://www.zlgan.top/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"author":"gzl"},{"title":"CNN(Pytorch)-Based Anomaly Detection","slug":"CNN-Based-Anomaly-Detection","date":"2022-04-21T03:34:31.722Z","updated":"2022-04-23T12:10:05.033Z","comments":true,"path":"2022/04/21/CNN-Based-Anomaly-Detection/","link":"","permalink":"https://www.zlgan.top/2022/04/21/CNN-Based-Anomaly-Detection/","excerpt":"","text":"CNN(Pytorch)-Based Anomaly Detection数据集：CIC-IDS2017 数据条数：250w 左右 数据特征：78 数据标签：14 攻击 + 1正常 使用两个一维卷积接一个全连接层 Project文件结构 Dataloder.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132import numpy as npimport pandas as pdfrom sklearn.model_selection import train_test_splitfrom sklearn import preprocessingimport time# 根据file读取数据def readData(file): print(&quot;Loading raw data...&quot;) raw_data = pd.read_csv(file, header=None, low_memory=False) return raw_data# 返回需要清除的行数listdef clearDirtyData(df): dropList = df[(df[14] == &quot;Nan&quot;) | (df[15] == &quot;Infinity&quot;)].index.tolist() return dropListdef lookData(raw_data): # 打印数据集的标签数据数量 # last_column_index = raw_data.shape[1] - 1 # print(raw_data[last_column_index].value_counts()) # 取出数据集标签部分 labels = raw_data.iloc[:, raw_data.shape[1] - 1:] # 多维数组转为以为数组 labels = labels.values.ravel() label_set = set(labels) return label_setdef separateData(raw_data): lists = raw_data.values.tolist() temp_lists = [] for i in range(0, 2): temp_lists.append([]) # 得到raw_data的数据标签集合 label_set = lookData(raw_data) # 将无序的数据标签集合转换为有序的list label_list = list(label_set) for i in range(0, len(lists)): # 得到所属标签的索引号 data_index = label_list.index(lists[i][len(lists[0]) - 1]) temp_lists[data_index].append(lists[i]) return temp_listsdef DataLoder(): acc_time = 0 start = time.time() data_1 = readData( &quot;/data/GanZhenliang/PycharmProjects/Anomaly_Detection/data/MachineLearningCVE/Monday-WorkingHours.pcap_ISCX.csv&quot;) data_1 = data_1.drop([0]) data_2 = readData( &quot;/data/GanZhenliang/PycharmProjects/Anomaly_Detection/data/MachineLearningCVE/Tuesday-WorkingHours.pcap_ISCX.csv&quot;) data_2 = data_2.drop([0]) data_3 = readData( &quot;/data/GanZhenliang/PycharmProjects/Anomaly_Detection/data/MachineLearningCVE/Wednesday-workingHours.pcap_ISCX.csv&quot;) data_3 = data_3.drop([0]) data_4 = readData( &quot;/data/GanZhenliang/PycharmProjects/Anomaly_Detection/data/MachineLearningCVE/Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv&quot;) data_4 = data_4.drop([0]) data_5 = readData( &quot;/data/GanZhenliang/PycharmProjects/Anomaly_Detection/data/MachineLearningCVE/Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv&quot;) data_5 = data_5.drop([0]) data_6 = readData( &quot;/data/GanZhenliang/PycharmProjects/Anomaly_Detection/data/MachineLearningCVE/Friday-WorkingHours-Morning.pcap_ISCX.csv&quot;) data_6 = data_6.drop([0]) data_7 = readData( &quot;/data/GanZhenliang/PycharmProjects/Anomaly_Detection/data/MachineLearningCVE/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv&quot;) data_7 = data_7.drop([0]) data_8 = readData( &quot;/data/GanZhenliang/PycharmProjects/Anomaly_Detection/data/MachineLearningCVE/Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv&quot;) data_8 = data_8.drop([0]) end = time.time() print(&#x27;数据加载时间：&#x27;, end - start, &#x27;s&#x27;) acc_time += end - start # print(&#x27;开始处理数据&#x27;) start = time.time() temp = [data_1, data_2, data_3, data_4, data_5, data_6, data_7, data_8] # temp = [data_7, data_8] data_all = pd.concat(temp) data_list_clear = clearDirtyData(data_all) data_all = data_all.drop(data_list_clear) # print(data_all[data_all.shape[1] - 1].value_counts()) data_all = data_all.sample(frac=1) print(data_all[data_all.shape[1] - 1].value_counts()) data_all[data_all.shape[1] - 1], attacks = pd.factorize(data_all[data_all.shape[1] - 1], sort=True) # data_all[78][data_all[78] != 0] = 1 print(data_all[data_all.shape[1] - 1].value_counts()) # ----------------------------------------降采样正常样本--------------------------------------- # data_all[78][data_all[78] != 0] = 1 # lists = separateData(data_all) # normal = pd.DataFrame(lists[0]) # normal = normal.sample(frac=0.5) # abnormal = pd.DataFrame(lists[1]) # temp = [normal, abnormal] # data_all = pd.concat(temp) # # print(data_all[data_all.shape[1] - 1].value_counts()) # ----------------------------------------数据处理，标准化、归一化、等--------------------------------------- features = data_all.iloc[:, :data_all.shape[1] - 1] labels = data_all.iloc[:, data_all.shape[1] - 1:] # 标准化.高斯分布(0,1) # features = preprocessing.scale(features) # 最小最大值标准化,最小0，最大1 features = preprocessing.minmax_scale(features, feature_range=(-3, 3)) # 归一化 # features = preprocessing.normalize(features, norm=&quot;l2&quot;) labels = labels.values.ravel() # labels[labels != 0] = 1 train_x, test_x, train_y, test_y = train_test_split(features, labels, test_size=0.3) # train_x, test_x, train_y, test_y = train_test_split(features, Y_onehot, test_size=0.3) end = time.time() print(&#x27;数据处理时间：&#x27;, end - start, &#x27;s&#x27;) acc_time += end - start return train_x, test_x, train_y, test_y, acc_time # print(&quot;训练集和目标值&quot;, train_x, train_y) # print(&quot;测试集和目标值&quot;, test_x, test_y) CNN_Pytorch.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197import pandas as pdimport torchimport torchvisionimport torch.nn as nnimport numpy as npimport matplotlib.pyplot as pltimport torch.utils.data as Datafrom Dataloder import DataLoderfrom sklearn import metricsimport seaborn as snsfrom sklearn.metrics import confusion_matrixepochs = 100batch_size = 128lr = 0.001train_data, test_data, train_label, test_label, acc_time = DataLoder()train_data = train_data.reshape(train_data.shape[0], 1, 78)test_data = test_data.reshape(test_data.shape[0], 1, 78)# 制作pytorch识别的数据集和定义模型train_data, train_label = torch.Tensor(train_data), torch.Tensor(train_label.T).long()test_data, test_label = torch.Tensor(test_data), torch.Tensor(test_label.T).long()train_dataset = Data.TensorDataset(train_data, train_label)test_dataset = Data.TensorDataset(test_data, test_label)# 制作Dataloder数据集，可迭代train_loader = Data.DataLoader(train_dataset, batch_size=128, shuffle=True)test_loader = Data.DataLoader(test_dataset, batch_size=128, shuffle=False)class CNN(nn.Module): def __init__(self): super(CNN, self).__init__() &#x27;&#x27;&#x27; 一般来说，卷积网络包括以下内容： 1.卷积层 2.神经网络 3.池化层 &#x27;&#x27;&#x27; self.conv1 = nn.Sequential( nn.Conv1d( in_channels=1, out_channels=128, kernel_size=2, # stride=1, # padding=1, # 边框补全，其计算公式=（kernel_size-1）/2=(5-1)/2=2 ), nn.BatchNorm1d(128), # 使每一层神经网络的输入保持相同分布的 nn.ReLU(), # 非线性激活层 nn.MaxPool1d(kernel_size=2), ) self.conv2 = nn.Sequential( nn.Conv1d( in_channels=128, out_channels=32, kernel_size=2, # stride=1, # padding=1, # 边框补全，其计算公式=（kernel_size-1）/2=(5-1)/2=2 ), nn.BatchNorm1d(32), nn.ReLU(), # 非线性激活层 nn.MaxPool1d(kernel_size=2), ) self.out = nn.Linear(576, 15) def forward(self, x): x = self.conv1(x) # 128,1,78 -&gt; 128 128 38 x = self.conv2(x) # 128 128 38 -&gt; 128 128 18 x = x.view(x.size(0), -1) # 128,576 output = self.out(x) return outputdevice = torch.device(&#x27;cuda:1&#x27; if torch.cuda.is_available() else &#x27;cpu&#x27;)model = CNN()model = model.to(device) # 传到GPU# 定义损失函数loss = torch.nn.CrossEntropyLoss()# 定义优化器optimizer = torch.optim.Adam(model.parameters(), lr=0.001)# 模型的训练损失（loss）和验证损失（val_loss），以及训练准确率（acc）和验证准确率（val_acc）可以使用绘图代码绘制出来def visualize(loss, val_loss, acc, val_acc): epochs = range(1, len(loss) + 1) plt.plot(epochs, loss, &#x27;bo&#x27;, label=&#x27;Training loss&#x27;) plt.plot(epochs, val_loss, &#x27;b&#x27;, label=&#x27;Validation loss&#x27;) plt.title(&#x27;Training and validatio loss&#x27;) plt.xlabel(&#x27;Epochs&#x27;) plt.ylabel(&#x27;Loss&#x27;) plt.legend() plt.savefig(&#x27;result_CNN_Pytorch_loss.png&#x27;, bbox_inches=&#x27;tight&#x27;) plt.show() plt.figure() plt.plot(epochs, acc, &#x27;bo&#x27;, label=&#x27;Training acc&#x27;) plt.plot(epochs, val_acc, &#x27;b&#x27;, label=&#x27;Validation acc&#x27;) plt.title(&#x27;Training and validatio accuracy&#x27;) plt.legend() plt.savefig(&#x27;result_CNN_Pytorch_acc.png&#x27;, bbox_inches=&#x27;tight&#x27;) plt.show()# 绘图def pltfigure(x, y, title, id, data): plt.subplot(2, 2, id) plt.plot(range(len(data)), data) plt.xlabel(x) plt.ylabel(y) plt.title(title, fontsize=10) # plt.show()def Confusion_matrix_hp(cm): cm = pd.DataFrame(cm) sns.heatmap(cm, annot=False, cmap=&#x27;GnBu&#x27;) plt.xlabel(&#x27;Real Label&#x27;) plt.ylabel(&#x27;Predict Label&#x27;) plt.savefig(&#x27;Confusion_CNN_Pytorch.png&#x27;, bbox_inches=&#x27;tight&#x27;) plt.show()# 训练def train(): model.train() batch_loss, correct, total = 0.0, 0.0, 0.0 for data, label in train_loader: data, label = data.to(device), label.to(device) model.zero_grad() output = model(data) l = loss(output, label) l.backward() optimizer.step() predict_label = torch.argmax(output, dim=1) correct += torch.sum(predict_label == label).cpu().item() total += len(label) batch_loss += l.cpu().item() return correct / total, batch_loss / len(train_loader)# 测试def test(): model.eval() batch_loss, correct, total = 0.0, 0.0, 0.0 predict = np.zeros((1, 1)) for data, label in test_loader: data, label = data.to(device), label.to(device) output = model(data) batch_loss += loss(output, label).cpu().item() predict_label = torch.argmax(output, dim=1) predict = np.append(predict, predict_label.data.cpu().numpy()) correct += torch.sum(predict_label == label).cpu().item() total += len(label) return correct / total, batch_loss / len(test_loader), predict[1:]# 主程序if __name__ == &quot;__main__&quot;: print(&#x27;training on: &#x27;, device) print(&#x27;batch_size:&#x27;, batch_size) print(&#x27;epochs:&#x27;, epochs) print(&#x27;learning_rate:&#x27;, lr) # plt.figure() train_acc_list, train_loss_list, val_acc_list, val_loss_list = [], [], [], [] for epoch in range(epochs): train_acc, train_loss = train() test_acc, test_loss, predict = test() print(&#x27;epoch %d: train acc: %.2f%% train loss:%.4f, test acc: %.2f%%, test loss:%.4f&#x27; % (epoch, 100 * train_acc, train_loss, 100 * test_acc, test_loss)) train_acc_list.append(train_acc) train_loss_list.append(train_loss) val_acc_list.append(test_acc) val_loss_list.append(test_loss) test_y = np.array(test_label) print(metrics.classification_report(test_y, predict)) # 最后一个epoch的报告 cm = confusion_matrix(test_y, predict) Confusion_matrix_hp(cm) # 绘图 pltfigure(x=&#x27;epoch&#x27;, y=&#x27;acc&#x27;, title=&#x27;epoch-train_acc&#x27;, id=1, data=train_acc_list) pltfigure(x=&#x27;epoch&#x27;, y=&#x27;loss&#x27;, title=&#x27;epoch-train_loss&#x27;, id=2, data=train_loss_list) pltfigure(x=&#x27;epoch&#x27;, y=&#x27;acc&#x27;, title=&#x27;epoch-val_acc&#x27;, id=3, data=val_acc_list) pltfigure(x=&#x27;epoch&#x27;, y=&#x27;loss&#x27;, title=&#x27;epoch-val_loss&#x27;, id=4, data=val_loss_list) plt.tight_layout() plt.savefig(&#x27;result_CNN_Pytorch.png&#x27;, bbox_inches=&#x27;tight&#x27;) plt.show() visualize(train_loss_list, val_loss_list, train_acc_list, val_acc_list) 结果：123456epochs = 100batch_size = 128lr = 0.001百分之一数据集训练集 70%验证集 30% 二分类：准确率，loss曲线 最后一个Epoch的报告，混淆矩阵： 15分类正常样本太多，各种攻击样本数量也非常不平衡，混淆矩阵没有任何意义 准确率，loss曲线 最后一个Epoch分类结果 precision recall f1-score support BENIGN 0.99 1.00 0.99 677804 Bot 1.00 0.35 0.52 556 DDoS 1.00 0.96 0.98 38298 DoS GoldenEye 0.99 0.96 0.98 3132 DoS Hulk 0.98 0.95 0.97 68882 DoS Slowhttptest 0.99 0.92 0.95 1625 DoS slowloris 0.96 0.99 0.98 1715 FTP-Patator 1.00 0.99 1.00 2310 Heartbleed 1.00 1.00 1.00 5 Infiltration 0.92 0.71 0.80 17 PortScan 0.99 1.00 1.00 47369 SSH-Patator 0.96 0.87 0.91 1783 Web Attack Brute Force 0.68 0.95 0.79 438 Web Attack Sql Injection 0.00 0.00 0.00 5 Web Attack XSS 0.26 0.03 0.05 201 accuracy 0.99 844140 macro avg 0.85 0.78 0.79 844140 weighted avg 0.99 0.76 0.99 844140","categories":[{"name":"入侵检测","slug":"入侵检测","permalink":"https://www.zlgan.top/categories/%E5%85%A5%E4%BE%B5%E6%A3%80%E6%B5%8B/"}],"tags":[{"name":"Anomaly Detection","slug":"Anomaly-Detection","permalink":"https://www.zlgan.top/tags/Anomaly-Detection/"},{"name":"机器学习","slug":"机器学习","permalink":"https://www.zlgan.top/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"CNN","slug":"CNN","permalink":"https://www.zlgan.top/tags/CNN/"}],"author":"gzl"},{"title":"KNN(自己写)-Based Anomaly Detection","slug":"KNN(自己写)-Based-Anomaly-Detection","date":"2022-04-21T02:44:32.281Z","updated":"2022-10-05T13:43:22.925Z","comments":true,"path":"2022/04/21/KNN(自己写)-Based-Anomaly-Detection/","link":"","permalink":"https://www.zlgan.top/2022/04/21/KNN(%E8%87%AA%E5%B7%B1%E5%86%99)-Based-Anomaly-Detection/","excerpt":"","text":"KNN-Based Anomaly Detection上一篇通过调用sklearn中的knn实现Anomaly Detection，听老师指正以及和同学讨论之后，觉得自己可能有点小偏差。 可能老师要求的做异常检测的KNN的正常的做法是： 数据集划分，训练集全部用正常样本，测试集异常正常都有 散点图中的横坐标还是index，纵坐标是测试样本和正常样本的最小k个距离的平均 这样，就可以通过，不断的取阈值来画出ROC曲线。每取一个阈值，阈值以上都判定为异常点，阈值以下判断为正常点，计算FPR和TPR 这样做不能通过调用函数来计算准确率，因为训练集只有一个类别 其实就是基于异常的入侵检测系统，用正常样本学习训练正常的规律特征，如果遇到大于预设阈值的样本则认为是异常，判定为入侵。 根据王伟老师上课讲解的，基于异常的入侵检测系统特点是：就两种类别，是不是异常。能检测出异常，但不知道是哪种异常。 好处是可以快速和有效的检测出入侵（异常）。 但是随着正常样本的增多，模型要不断进行学习和更新。 Project文件结构 Dataloder.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131import numpy as npimport pandas as pdfrom sklearn.model_selection import train_test_splitfrom sklearn import preprocessingimport time# 根据file读取数据def readData(file): print(&quot;Loading raw data...&quot;) raw_data = pd.read_csv(file, header=None, low_memory=False) return raw_data# 返回需要清除的行数listdef clearDirtyData(df): dropList = df[(df[14] == &quot;Nan&quot;) | (df[15] == &quot;Infinity&quot;)].index.tolist() return dropListdef lookData(raw_data): # 打印数据集的标签数据数量 # last_column_index = raw_data.shape[1] - 1 # print(raw_data[last_column_index].value_counts()) # 取出数据集标签部分 labels = raw_data.iloc[:, raw_data.shape[1] - 1:] # 多维数组转为以为数组 labels = labels.values.ravel() label_set = set(labels) return label_setdef separateData(raw_data): lists = raw_data.values.tolist() temp_lists = [] for i in range(0, 2): temp_lists.append([]) # 得到raw_data的数据标签集合 label_set = lookData(raw_data) # 将无序的数据标签集合转换为有序的list label_list = list(label_set) for i in range(0, len(lists)): # 得到所属标签的索引号 data_index = label_list.index(lists[i][len(lists[0]) - 1]) temp_lists[data_index].append(lists[i]) return temp_listsdef DataLoder(): acc_time = 0 # ----------------------------------------读取数据，合并到一个DataFrame--------------------------------------- start = time.time() data_1 = readData(&quot;data\\MachineLearningCVE\\Monday-WorkingHours.pcap_ISCX.csv&quot;) data_1 = data_1.drop([0]) # 去掉非数值类型的第一行 data_2 = readData(&quot;data\\MachineLearningCVE\\Tuesday-WorkingHours.pcap_ISCX.csv&quot;) data_2 = data_2.drop([0]) data_3 = readData(&quot;data\\MachineLearningCVE\\Wednesday-workingHours.pcap_ISCX.csv&quot;) data_3 = data_3.drop([0]) data_4 = readData(&quot;data\\MachineLearningCVE\\Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv&quot;) data_4 = data_4.drop([0]) data_5 = readData(&quot;data\\MachineLearningCVE\\Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv&quot;) data_5 = data_5.drop([0]) data_6 = readData(&quot;data\\MachineLearningCVE\\Friday-WorkingHours-Morning.pcap_ISCX.csv&quot;) data_6 = data_6.drop([0]) data_7 = readData(&quot;data\\MachineLearningCVE\\Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv&quot;) data_7 = data_7.drop([0]) data_8 = readData(&quot;data\\MachineLearningCVE\\Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv&quot;) data_8 = data_8.drop([0]) end = time.time() print(&#x27;数据加载时间：&#x27;, end - start, &#x27;s&#x27;) acc_time += end - start # ----------------------------------------处理数据--------------------------------------- # print(&#x27;开始处理数据&#x27;) start = time.time() temp = [data_1, data_2, data_3, data_4, data_5, data_6, data_7, data_8] # temp = [data_7, data_8] data_all = pd.concat(temp) # 删除Nan和Infinity所在行 data_list_clear = clearDirtyData(data_all) data_all = data_all.drop(data_list_clear) print(data_all[data_all.shape[1] - 1].value_counts()) # 数据量太大，可以降采样frac=抽取百分比,现在是百分之0.1 data_all = data_all.sample(frac=0.001) print(data_all[data_all.shape[1] - 1].value_counts()) # 把最后一列lable转化为数值 data_all[data_all.shape[1] - 1], attacks = pd.factorize(data_all[data_all.shape[1] - 1], sort=True) # data_all[78][data_all[78] != 0] = 1 # print(data_all[data_all.shape[1] - 1].value_counts()) # ----------------------------------------划分正常，异常样本--------------------------------------- features = data_all.iloc[:, :data_all.shape[1] - 1].values labels = data_all.iloc[:, data_all.shape[1] - 1:].values # 标准化.高斯分布(0,1) features = preprocessing.scale(features) # 最小最大值标准化,最小0，最大1 # features = preprocessing.minmax_scale(features) # 归一化 # features = preprocessing.normalize(features, norm=&quot;l2&quot;) data_all = np.c_[features, labels] # 变成二分类问题 data_all[:, 78][data_all[:, 78] != 0] = 1 data_all = pd.DataFrame(data_all) lists = separateData(data_all) normal = pd.DataFrame(lists[0]) # 取7/8的正常样本作为训练集，剩下的1/8和异常样本一起作为测试集 train = normal.iloc[0:int(normal.shape[0] / 8) * 7, :] normal = normal.iloc[int(normal.shape[0] / 8) * 7:normal.shape[0], :] abnormal = pd.DataFrame(lists[1]) temp = [normal, abnormal] data_all = pd.concat(temp) data_all = data_all.sample(frac=1) test_x = data_all.iloc[:, :data_all.shape[1] - 1].values test_y = data_all.iloc[:, data_all.shape[1] - 1:].values.ravel() train_x = train.iloc[:, :data_all.shape[1] - 1].values train_y = train.iloc[:, data_all.shape[1] - 1:].values.ravel() end = time.time() print(&#x27;数据处理时间：&#x27;, end - start, &#x27;s&#x27;) acc_time += end - start return train_x, test_x, train_y, test_y, acc_time KNN.py1234567891011121314151617181920212223242526272829303132333435363738394041424344import numpy as npfrom collections import Counterclass KNN: def __init__(self, k): assert k &gt;= 1, &#x27;k must be valid&#x27; self.k = k self._x_train = None self._y_train = None def fit(self, x_train, y_train): self._x_train = x_train self._y_train = y_train return self def _predict(self, x): d = [np.sqrt(np.sum((x_i - x) ** 2)) for x_i in self._x_train] nearest = np.argsort(d) top_k = [self._y_train[i] for i in nearest[:self.k]] votes = Counter(top_k) return votes.most_common(1)[0][0] def predict(self, X_predict): y_predict = [self._predict(x1) for x1 in X_predict] return np.array(y_predict) def distance(self, x): d = [np.sqrt(np.sum((x_i - x) ** 2)) for x_i in self._x_train] d = np.array(d) d = np.sort(d) d = d[:self.k].mean() return d def Distance(self, X_predict): res = [self.distance(x1) for x1 in X_predict] return np.array(res) def __repr__(self): return &#x27;knn(k=%d):&#x27; % self.k def score(self, x_test, y_test): y_predict = self.predict(x_test) return sum(y_predict == y_test) / len(x_test) main.py12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182from matplotlib import pyplot as pltfrom sklearn.metrics import accuracy_scorefrom KNN import KNNfrom Dataloder import DataLoderimport numpy as npimport pandas as pdimport timedef visualize(X): for i in range(len(X[2])): if X[2][i] == 0: s1 = plt.scatter(X[0][i], X[1][i], s=2, color=&#x27;b&#x27;, marker=&#x27;o&#x27;) elif X[2][i] == 1: s2 = plt.scatter(X[0][i], X[1][i], s=2, color=&#x27;r&#x27;, marker=&#x27;x&#x27;) plt.xlabel(&#x27;Index&#x27;) plt.ylabel(&#x27;Distance&#x27;) plt.ylim((0, 30)) plt.title(&quot;Index_Distance&quot;) plt.legend((s1, s2), (&#x27;Normal&#x27;, &#x27;Abnormal&#x27;), loc=&#x27;best&#x27;) plt.savefig(&#x27;san.png&#x27;, bbox_inches=&#x27;tight&#x27;) plt.show()def roc(X): normal = 0 X_size = X.shape[1] # roc_rate = np.zeros((2, X_size)) roc_rate = np.zeros((2, 10000)) # 点的数量 for i in range(X_size): if X[2][i] == 0: normal += 1 abnormal = X_size - normal # max_dis = X[1].max() max_dis = 30 for j in range(10000): # 范围也是点的数量，越大时间越长，roc曲线越准确 threshold = max_dis / 10000 * j normal1 = 0 abnormal1 = 0 for k in range(X_size): if X[1][k] &gt; threshold and X[2][k] == 0: normal1 += 1 if X[1][k] &gt; threshold and X[2][k] != 0: abnormal1 += 1 # ROC曲线：横轴误报率，即阈值以上正常点 / 全体正常的点；纵轴检测率，即阈值以上异常点 / 全体异常点 roc_rate[0][j] = normal1 / normal # 阈值以上正常点/全体正常的点 roc_rate[1][j] = abnormal1 / abnormal # 阈值以上异常点/全体异常点 # roc_rate[0][j] = (abnormal-abnormal1) / abnormal # 阈值以下异常点/全体异常的点 # roc_rate[1][j] = (normal-normal1) / normal # 阈值以下正常点/全体正常点 plt.figure() plt.scatter(roc_rate[0], roc_rate[1], edgecolors=&#x27;None&#x27;, s=1, alpha=1) plt.savefig(&#x27;Roc0.png&#x27;, bbox_inches=&#x27;tight&#x27;) plt.show()if __name__ == &quot;__main__&quot;: train_x, test_x, train_y, test_y, acc_time = DataLoder() model = KNN(k=1) start = time.time() model.fit(train_x, train_y) end = time.time() print(&#x27;训练时间：&#x27;, end - start, &#x27;s&#x27;) acc_time += end - start start = time.time() # 可以注释掉这一行，因为训练集只有一个类别 # predict = model.predict(test_x) distance = model.Distance(test_x) result = np.zeros((len(test_x), 3)) for i in range(len(test_x)): # 序号 最小欧氏距离 测试集数据类别 result[i] = i + 1, distance[i], test_y[i] # 矩阵转置 result = np.transpose(result) visualize(result) roc(result) end = time.time() print(&#x27;预测时间：&#x27;, end - start, &#x27;s&#x27;) acc_time += end - start print(&#x27;总时间：&#x27;, acc_time, &#x27;s&#x27;) 运行结果可视化：散点图： ROC：10000个点 100000个点 可以把画ROC的函数改改，再改改main函数，让多条ROC曲线画在同一张画布，用来比较不同K值下的分类性能。 改之后main.py如下 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384from matplotlib import pyplot as pltfrom sklearn.metrics import accuracy_scorefrom KNN import KNNfrom Dataloder import DataLoderimport numpy as npimport pandas as pdimport timedef visualize(X, K): plt.figure(2) for i in range(len(X[2])): if X[2][i] == 0: s1 = plt.scatter(X[0][i], X[1][i], s=2, color=&#x27;b&#x27;, marker=&#x27;o&#x27;) elif X[2][i] == 1: s2 = plt.scatter(X[0][i], X[1][i], s=2, color=&#x27;r&#x27;, marker=&#x27;x&#x27;) plt.xlabel(&#x27;Index&#x27;) plt.ylabel(&#x27;Distance&#x27;) plt.ylim((0, 30)) plt.title(&quot;Index_Distance&quot;) plt.legend((s1, s2), (&#x27;Normal&#x27;, &#x27;Abnormal&#x27;), loc=&#x27;best&#x27;) img = &#x27;san_K=&#x27; + str(K) plt.savefig(img, bbox_inches=&#x27;tight&#x27;) plt.show()def roc(X, K): normal = 0 X_size = X.shape[1] # roc_rate = np.zeros((2, X_size)) roc_rate = np.zeros((2, 10000)) # 点的数量 for i in range(X_size): if X[2][i] == 0: normal += 1 abnormal = X_size - normal # max_dis = X[1].max() max_dis = 30 for j in range(10000): # 范围也是点的数量，越大时间越长，roc曲线越准确 threshold = max_dis / 10000 * j normal1 = 0 abnormal1 = 0 for k in range(X_size): if X[1][k] &gt; threshold and X[2][k] == 0: normal1 += 1 if X[1][k] &gt; threshold and X[2][k] != 0: abnormal1 += 1 # ROC曲线：横轴误报率，即阈值以上正常点 / 全体正常的点；纵轴检测率，即阈值以上异常点 / 全体异常点 roc_rate[0][j] = normal1 / normal # 阈值以上正常点/全体正常的点 roc_rate[1][j] = abnormal1 / abnormal # 阈值以上异常点/全体异常点 # roc_rate[0][j] = (abnormal-abnormal1) / abnormal # 阈值以下异常点/全体异常的点 # roc_rate[1][j] = (normal-normal1) / normal # 阈值以下正常点/全体正常点 plt.plot(roc_rate[0], roc_rate[1], label=str(K))def Res(model, test_x): distance = model.Distance(test_x) result = np.zeros((len(test_x), 3)) for i in range(len(test_x)): # 序号 最小欧氏距离 测试集数据类别 result[i] = i + 1, distance[i], test_y[i] # 矩阵转置 result = np.transpose(result) return resultif __name__ == &quot;__main__&quot;: train_x, test_x, train_y, test_y, acc_time = DataLoder() start = time.time() # 散点图和ROC曲线不能同时画，因为散点图是每个K画一个，ROC是多个K画一张图,我用plt.figure()指定了半天没整明白 for i in range(1, 3): model = KNN(k=i) model.fit(train_x, train_y) result = Res(model, test_x) # 散点图和ROC曲线不能同时画 # visualize(result, i) roc(result, i) plt.legend() plt.savefig(&#x27;Roc.png&#x27;, bbox_inches=&#x27;tight&#x27;) plt.show() end = time.time() print(&#x27;总时间：&#x27;, acc_time, &#x27;s&#x27;) 要多画几条，就改循环中i的范围","categories":[{"name":"入侵检测","slug":"入侵检测","permalink":"https://www.zlgan.top/categories/%E5%85%A5%E4%BE%B5%E6%A3%80%E6%B5%8B/"}],"tags":[{"name":"Anomaly Detection","slug":"Anomaly-Detection","permalink":"https://www.zlgan.top/tags/Anomaly-Detection/"},{"name":"机器学习","slug":"机器学习","permalink":"https://www.zlgan.top/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"KNN","slug":"KNN","permalink":"https://www.zlgan.top/tags/KNN/"}],"author":"gzl"},{"title":"KNN_Sklearn-Based Anomaly Detection","slug":"KNN_Sklearn-Based Anomaly Detection","date":"2022-04-21T01:20:51.092Z","updated":"2022-04-21T05:24:54.258Z","comments":true,"path":"2022/04/21/KNN_Sklearn-Based Anomaly Detection/","link":"","permalink":"https://www.zlgan.top/2022/04/21/KNN_Sklearn-Based%20Anomaly%20Detection/","excerpt":"","text":"KNN_Sklearn-Based Anomaly Detection这里调用Sklearn中的KNN，实现对ICI-IDS2017数据集的异常检测。 数据处理为二分类任务。 如果想做多分类也可以，在数据处理Dataloder.py注释掉: 12# 变成二分类问题，0正常，1异常# labels[labels != 0] = 1 在main注释掉: 1# roc_sklearn(test_y, probility[:, 1]) 画散点图函数def visualize(X)中把添加颜色的循环注释掉，改为按照标签数值设置点颜色： 1234# for d in range(len(X[2])):# colors.append(color[int(X[2][d])])plt.figure()plt.scatter(X[0], X[1], c=X[2], edgecolors=&#x27;None&#x27;, s=2, alpha=1) 主要思路是： 调用Sklearn中KNN实现二分类（异常&#x2F;正常），训练集测试集都有俩个类别 画出测试集结果的散点图 散点图的横坐标是index（序号，第几个） 散点图纵坐标是离它最近的K个点距离的平均值 画出ROC曲线，调用sklearn中roc_curve()函数直接返回，tpr，fpr，阈值 ROC曲线的横纵坐标就是FPR和TPR，然后调用plt.plot() 因为老师上课要求必须得有散点图和ROC曲线，所以就这么搞了。听老师指正以及和同学讨论之后，觉得自己可能有点小偏差。 可能老师讲的做异常检测的KNN的正常的做法是：（下一篇更新） 数据集划分，训练集全部用正常样本，测试集异常正常都有 散点图中的横坐标还是index，纵坐标是测试样本和正常样本的最小k个距离的平均 这样，就可以通过，不断的取阈值来画出ROC曲线。每取一个阈值，阈值以上都判定为异常点，阈值以下判断为正常点，计算FPR和TPR 这样做不能通过调用函数来计算准确率，因为训练集只有一个类别 Project文件结构 Dataloder.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127import pandas as pdfrom sklearn.model_selection import train_test_splitfrom sklearn import preprocessingimport time# 根据file读取数据def readData(file): print(&quot;Loading raw data...&quot;) raw_data = pd.read_csv(file, header=None, low_memory=False) return raw_data# 返回需要清除的行数listdef clearDirtyData(df): dropList = df[(df[14] == &quot;Nan&quot;) | (df[15] == &quot;Infinity&quot;)].index.tolist() return dropListdef lookData(raw_data): # 打印数据集的标签数据数量 # last_column_index = raw_data.shape[1] - 1 # print(raw_data[last_column_index].value_counts()) # 取出数据集标签部分 labels = raw_data.iloc[:, raw_data.shape[1] - 1:] # 多维数组转为以为数组 labels = labels.values.ravel() label_set = set(labels) return label_setdef separateData(raw_data): lists = raw_data.values.tolist() temp_lists = [] for i in range(0, 2): temp_lists.append([]) # 得到raw_data的数据标签集合 label_set = lookData(raw_data) # 将无序的数据标签集合转换为有序的list label_list = list(label_set) for i in range(0, len(lists)): # 得到所属标签的索引号 data_index = label_list.index(lists[i][len(lists[0]) - 1]) temp_lists[data_index].append(lists[i]) return temp_listsdef DataLoder(): acc_time = 0 # ----------------------------------------读取数据，合并到一个DataFrame--------------------------------------- start = time.time() data_1 = readData(&quot;data\\MachineLearningCVE\\Monday-WorkingHours.pcap_ISCX.csv&quot;) data_1 = data_1.drop([0]) data_2 = readData(&quot;data\\MachineLearningCVE\\Tuesday-WorkingHours.pcap_ISCX.csv&quot;) data_2 = data_2.drop([0]) data_3 = readData(&quot;data\\MachineLearningCVE\\Wednesday-workingHours.pcap_ISCX.csv&quot;) data_3 = data_3.drop([0]) data_4 = readData(&quot;data\\MachineLearningCVE\\Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv&quot;) data_4 = data_4.drop([0]) data_5 = readData(&quot;data\\MachineLearningCVE\\Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv&quot;) data_5 = data_5.drop([0]) data_6 = readData(&quot;data\\MachineLearningCVE\\Friday-WorkingHours-Morning.pcap_ISCX.csv&quot;) data_6 = data_6.drop([0]) data_7 = readData(&quot;data\\MachineLearningCVE\\Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv&quot;) data_7 = data_7.drop([0]) data_8 = readData(&quot;data\\MachineLearningCVE\\Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv&quot;) data_8 = data_8.drop([0]) end = time.time() print(&#x27;数据加载时间：&#x27;, end - start, &#x27;s&#x27;) acc_time += end - start # ----------------------------------------处理数据--------------------------------------- # print(&#x27;开始处理数据&#x27;) start = time.time() temp = [data_1, data_2, data_3, data_4, data_5, data_6, data_7, data_8] # temp = [data_7, data_8] data_all = pd.concat(temp) # 删除Nan和Infinity所在行 data_list_clear = clearDirtyData(data_all) data_all = data_all.drop(data_list_clear) print(data_all[data_all.shape[1] - 1].value_counts()) # 数据量太大，可以降采样frac=抽取百分比 data_all = data_all.sample(frac=0.01) print(data_all[data_all.shape[1] - 1].value_counts()) # 把最后一列lable转化为数值 data_all[data_all.shape[1] - 1], attacks = pd.factorize(data_all[data_all.shape[1] - 1], sort=True) # data_all[78][data_all[78] != 0] = 1 # print(data_all[data_all.shape[1] - 1].value_counts()) # ----------------------------------------降采样正常样本--------------------------------------- # data_all[78][data_all[78] != 0] = 1 # lists = separateData(data_all) # normal = pd.DataFrame(lists[0]) # normal = normal.sample(frac=0.5) # abnormal = pd.DataFrame(lists[1]) # temp = [normal, abnormal] # data_all = pd.concat(temp) # # print(data_all[data_all.shape[1] - 1].value_counts()) # ----------------------------------------数据处理，标准化、归一化、等--------------------------------------- features = data_all.iloc[:, :data_all.shape[1] - 1].values labels = data_all.iloc[:, data_all.shape[1] - 1:].values # 标准化.高斯分布(0,1) features = preprocessing.scale(features) # 最小最大值标准化,最小0，最大1，可以指定范围 # features = preprocessing.minmax_scale(features) # 归一化 # features = preprocessing.normalize(features, norm=&quot;l2&quot;) # features = pd.DataFrame(features) labels = labels.ravel() # 变成二分类问题，0正常，1异常 labels[labels != 0] = 1 train_x, test_x, train_y, test_y = train_test_split(features, labels, test_size=0.3) end = time.time() print(&#x27;数据处理时间：&#x27;, end - start, &#x27;s&#x27;) acc_time += end - start return train_x, test_x, train_y, test_y, acc_time main.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188import scipy.io as sioimport numpy as npimport pandas as pdfrom sklearn.metrics import accuracy_scorefrom sklearn.model_selection import train_test_splitfrom sklearn.metrics import roc_curve, aucimport matplotlib.pyplot as pltfrom sklearn import metricsfrom sklearn.decomposition import PCAfrom Dataloder import DataLoderfrom sklearn.model_selection import GridSearchCVfrom sklearn.model_selection import cross_val_scorefrom sklearn.neighbors import KNeighborsClassifierfrom sklearn import datasetsimport seaborn as snsimport timefrom sklearn.metrics import confusion_matrixfrom sklearn.ensemble import AdaBoostClassifierdef visualize(X): # dd = [] # for d in range(len(X[0])): # if X[1][d] &gt; 10: # dd.append(d) # # X = np.delete(X, d, axis=1) # # X[1][d] = avg # X = np.delete(X, dd, axis=1) color = [&#x27;b&#x27;, &#x27;r&#x27;] # 设置颜色 colors = [] marker = [&#x27;o&#x27;, &#x27;x&#x27;] # 设置点的形状 makers = [] for d in range(len(X[2])): colors.append(color[int(X[2][d])]) plt.figure() plt.scatter(X[0], X[1], c=colors, edgecolors=&#x27;None&#x27;, s=2, alpha=1) # plt.ylim((0, 0.5)) plt.savefig(&#x27;result.png&#x27;, bbox_inches=&#x27;tight&#x27;) plt.show()def roc_sklearn(true, pro): # fpr, tpr, thresholds = roc_curve(true, pro, pos_label=1, drop_intermediate=False) fpr, tpr, thresholds = roc_curve(true, pro, pos_label=1, drop_intermediate=True) print(&quot;FPR , TPR , thresholds&quot;) for i, value in enumerate(thresholds): print(&quot;%f %f %f&quot; % (fpr[i], tpr[i], value)) roc_auc = auc(fpr, tpr) plt.plot(fpr, tpr, label=&#x27;ROC (area = &#123;0:.4f&#125;)&#x27;.format(roc_auc), lw=1, linestyle=&quot;-&quot;, color=&#x27;red&#x27;) plt.plot([0, 1], [0, 1], &#x27;--&#x27;, color=(0.6, 0.6, 0.6), label=&#x27;Luck&#x27;) # 画对角线 plt.xlim([-0.02, 1.05]) plt.ylim([-0.02, 1.05]) plt.xlabel(&#x27;False Positive Rate&#x27;) plt.ylabel(&#x27;True Positive Rate&#x27;) plt.title(&#x27;ROC Curve&#x27;) plt.legend(loc=&quot;lower right&quot;) plt.savefig(&#x27;roc.png&#x27;, bbox_inches=&#x27;tight&#x27;) plt.show()def roc(data_set): normal = 0 data_set_size = data_set.shape[1] # roc_rate = np.zeros((2, data_set_size)) roc_rate = np.zeros((2, 100000)) for i in range(data_set_size): if data_set[2][i] == 0: normal += 1 abnormal = data_set_size - normal # max_dis = data_set[1].max()/10 max_dis = 10 for j in range(100000): threshold = max_dis / 100000 * j normal1 = 0 abnormal1 = 0 for k in range(data_set_size): if data_set[1][k] &gt; threshold and data_set[2][k] == 0: normal1 += 1 if data_set[1][k] &gt; threshold and data_set[2][k] != 0: abnormal1 += 1 # ROC曲线：横轴误报率，即阈值以上正常点 / 全体正常的点；纵轴检测率，即阈值以上异常点 / 全体异常点 roc_rate[0][j] = normal1 / normal # 阈值以上正常点/全体正常的点 roc_rate[1][j] = abnormal1 / abnormal # 阈值以上异常点/全体异常点 # roc_rate[0][j] = (abnormal-abnormal1) / abnormal # 阈值以下异常点/全体异常的点 # roc_rate[1][j] = (normal-normal1) / normal # 阈值以下正常点/全体正常点 return roc_ratedef Confusion_matrix_hp(cm): cm = pd.DataFrame(cm) sns.heatmap(cm, annot=True, cmap=&#x27;GnBu&#x27;) plt.xlabel(&#x27;Real Label&#x27;) plt.ylabel(&#x27;Predict Label&#x27;) plt.savefig(&#x27;Confusion.png&#x27;, bbox_inches=&#x27;tight&#x27;) plt.show()def Confuse(y_true, y_pred): maxtrix = confusion_matrix(y_true, y_pred) plt.matshow(maxtrix) plt.colorbar() plt.xticks(np.arange(maxtrix.shape[1])) plt.yticks(np.arange(maxtrix.shape[1])) plt.show()if __name__ == &quot;__main__&quot;: train_x, test_x, train_y, test_y, acc_time = DataLoder() # x = train_x # 特征 # y = train_y # 标签 # # r_range = range(1, 30) # n_neighbors的取值范围 # r_list = list() # # start = time.time() # for r in r_range: # knn = KNeighborsClassifier(n_neighbors=r) # scores = cross_val_score(knn, x, y, cv=5, scoring=&quot;accuracy&quot;) # r_list.append(scores.mean()) # 5折交叉验证的平均值 # end = time.time() # print(&#x27;选择最佳K值时间：&#x27;, end - start, &#x27;s&#x27;) # acc_time += end - start # plt.plot(r_range, r_list) # plt.savefig(&#x27;K.png&#x27;, bbox_inches=&#x27;tight&#x27;) # plt.show() # print(&#x27;最佳K值:&#x27;, (r_list.index(max(r_list))) + 1) # best_K = (r_list.index(max(r_list))) + 1 # model = LogisticRegression() # model = RandomForestClassifier() # model = XGBClassifier() # model = KNeighborsClassifier(n_neighbors=best_K) model = KNeighborsClassifier(n_neighbors=1) # model = AdaBoostClassifier(n_estimators=5) start = time.time() model.fit(train_x, train_y) end = time.time() print(&#x27;训练时间：&#x27;, end - start, &#x27;s&#x27;) acc_time += end - start start = time.time() predict = model.predict(test_x) end = time.time() print(&#x27;预测时间：&#x27;, end - start, &#x27;s&#x27;) acc_time += end - start print(&#x27;总时间：&#x27;, acc_time, &#x27;s&#x27;) probility = model.predict_proba(test_x) # score = model.score(test_x, test_y) # print(&#x27;predict=&#x27;, predict) # print(&#x27;Accuracy:&#x27;, score) print(&#x27;Accuracy:&#x27;, accuracy_score(test_y, predict)) print(metrics.classification_report(test_y, predict)) distances, indices = model.kneighbors(X=test_x) test_size = len(test_x) result = np.zeros((test_size, 3)) for i in range(test_size): # 序号 最小欧氏距离 测试集数据类别 result[i] = i + 1, distances[i].mean(), test_y[i] # 矩阵转置 result = np.transpose(result) visualize(result) roc_sklearn(test_y, probility[:, 1]) # roc_rate = roc(result) # plt.figure() # plt.scatter(roc_rate[0], roc_rate[1], edgecolors=&#x27;None&#x27;, s=1, alpha=1) # plt.show() cm = confusion_matrix(test_y, predict) # cm = pd.crosstab(predict, test_y) Confusion_matrix_hp(cm) # list_diag = np.diag(cm) # list_raw_sum = np.sum(cm, axis=1) # print(&quot;Predict accuracy of the decisionTree: &quot;, np.mean(list_diag) / np.mean(list_raw_sum)) # Confuse(test_y,predict) # ----------------------------------------降维可视化--------------------------------------- # pca = PCA(n_components=2) # newData = pca.fit_transform(test_x) # plt.figure() # plt.scatter(newData[:, 0], newData[:, 1], c=test_y, s=50) # plt.show() 运行结果可视化：k折交叉验证选取K值：代码里注释掉了，每选一次等于预测运行k次，非常耗时 散点图：（可以在代码里限制一下y轴范围） 12345678910def visualize(X): color = [&#x27;b&#x27;, &#x27;r&#x27;] # 设置颜色 colors = [] for d in range(len(X[2])): colors.append(color[int(X[2][d])]) plt.figure() plt.scatter(X[0], X[1], c=X[2], edgecolors=&#x27;None&#x27;, s=2, alpha=1) plt.ylim(0, 10) # 限制y轴 plt.savefig(&#x27;result.png&#x27;, bbox_inches=&#x27;tight&#x27;) plt.show() 限制后： Roc曲线： 混淆矩阵：","categories":[{"name":"入侵检测","slug":"入侵检测","permalink":"https://www.zlgan.top/categories/%E5%85%A5%E4%BE%B5%E6%A3%80%E6%B5%8B/"}],"tags":[{"name":"Anomaly Detection","slug":"Anomaly-Detection","permalink":"https://www.zlgan.top/tags/Anomaly-Detection/"},{"name":"机器学习","slug":"机器学习","permalink":"https://www.zlgan.top/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"KNN","slug":"KNN","permalink":"https://www.zlgan.top/tags/KNN/"}],"author":"gzl"},{"title":"“火焰”病毒","slug":"“火焰”病毒","date":"2022-03-29T14:12:08.819Z","updated":"2022-03-29T14:34:46.705Z","comments":true,"path":"2022/03/29/“火焰”病毒/","link":"","permalink":"https://www.zlgan.top/2022/03/29/%E2%80%9C%E7%81%AB%E7%84%B0%E2%80%9D%E7%97%85%E6%AF%92/","excerpt":"","text":"“火焰”病毒一、概述 2012年5月，卡巴斯基实验室发现了一种功能强大、结构复杂的计算机病毒，该病毒的攻击目标仅为中东和部分非洲国家的计算机系统。由于该病毒的一个主要模块中含有“Flame”字样，所以卡巴斯基实验室将其命名为“火焰”（Flame）病毒。 火焰病毒是一种集成了渗透扩散、预留后门、信息窃取等功能的间谍软件。通过分析火焰病毒的部分样本，安全专家发现该病毒可能在2008年就已经存在，秘密地潜伏了5年。在这些年中，火焰病毒利用模块化程度高的特性，不断更新程序模块，强化功能。目前获取的火焰病毒样本中包含了多个特定功能的库文件，如SSL&#x2F;SSH加密通信库、压缩库、加密库、网络嗅探库、数据解析库、SQLite3文本型数据库和Lua脚本解释库等，大小有20MB左右，这和之前发现的各种病毒软件有很大的区别。火焰病毒程序极其复杂，代码量达到65万行，是普通间谍病毒软件的100倍，以复杂程度和功能来衡量，Flame被包括世界电信联盟等官方以及卡巴斯基等国际权威厂商认定为迄今为止最复杂、最危险、最致命的病毒的计算机病毒之一。 火焰病毒的攻击模型：火焰病毒的攻击方法是一套清晰的攻击模型，主要分为四步： 发起中间人攻击； 劫持篡改通信内容； 伪造数字证书； 假冒签名伪造通信内容。 二、技术分析“火焰”是一个模块化的、可扩展和更新的恶意程序，根据自身特性主要从事以下活动：扫描网络资源，窃取信息；通过SSH和HTTPS协议与C&amp;C服务器进行通讯；同时使用内核模式和用户模式逻辑，使用WindowsAPC调用和线程启动操纵将其内部功能复杂化，将代码注入关键的进程中；作为Winlogon.exe的一部分被加载，之后注入到Explorer and Services；创建屏幕截图，记录音频会话；在Windows XP，Windows Vista和Windows7系统上运行；利用打印程序和快捷方式漏洞，使用SQLite存储收集到的信息；使用定制的DB作为攻击模块（表明该恶意代码的模块性和可扩展性）；使用经PE加密的信息资源。 2.1文件组成 火焰病毒包含或产生的各种类型文件如图1所示。 （1）病毒模块文件火焰病毒的可执行模块包括mssecmgr.ocx、msglu32.ocx、nteps32.ocx、advnetcfg.ocx和soapr32.ocx等。其中，mssecmgr.ocx为主模块程序，由该模块完成病毒的执行，并将其他各模块注入系统进程。火焰病毒的模块文件存放在%windir%\\system32目录下；其他通过更新升级下载的模块文件也存放在该目录下。 （2）病毒配置文件火焰病毒的配置数据文件包括ccalc32.sys和boot32drv.sys，其存放位置在%windir%\\system32目录下。 （3）病毒运行时产生的文件火焰病毒在运行时将产生记录执行情况的日志文件、包含各类被感染主机信息的信息收集文件、包含窃取用户信息的数据文件和SQLite3数据库文件等，以上文件主要存放在%windir%\\temp、%windir%和%windir%\\system32等目录下。 火焰病毒具有极强的防护能力，其模块文件、配置文件、日志文件和数据文件等都采用压缩算法进行压缩，并且经过加密算法和密钥进行加密，使用常用的安全软件无法检测出病毒的程序文件。 2.2传播途径&amp;启动过程传播途径“火焰”病毒的传播途径主要包括：网络共享捕获管理员信息；Windows打印后台服务远程代码执行漏洞（CVE-2010-2729）；通过移动存储设备进行感染传播；通过Windows的快捷方式’LNK&#x2F;PIF’的自动文件执行漏洞（CVE-2010-2568）进行传播。 启动过程该病毒可以采用以下两种不同的方法启动： 1）在注册表中添加自启动方式 HKLM_SYSTEM\\CurrentControlSet\\Control\\LsaAuthenticationPackages&#x3D;mssecmgr.ocx 2）使用如下命令，从rundll32运行： start&#x2F;wait rundll32.exec:\\windows\\system32\\mssecmgr.ocx,DDEnumCallback 火焰病毒的主模块程序是mssecmgr.ocx，系统在启动时加载执行mssecmgr.ocx，mssecmgr.ocx从其资源中提取火焰病毒各程序模块，通过解密、注入等方式将各模块注入winlogon.exe、services.exe、explorer.exe和iexplorer.exe等系统进程，其基本的时序关系如图2所示。 详细过程：在启动时，火焰病毒会多次加载其功能模块，mssecmgr.ocx文件作为LSA身份验证包被加载，大概2分钟后，advnetcfg.ocx文件通过services.exe进程被加载，每2~3分钟重复3次；大约2分钟之后，services.exe从mssecmgr.ocx加载nteps32.ocx，然后winlogon.exe进程也加载nteps32.ocx文件；该文件被多次加载，同时explorer.exe开始创建5个iexplore进程，紧接着创建wpgfilter.dat文件；2分钟后，ccalc32.sys被service.exe写入，随后在1分钟内winlogon.exe对其进行加载，接下来mssecmgr.ocx被复制到wavsup3.drv；然后boot32drv.sys被services.exe加载。 2.3程序模块注入火焰病毒的多个程序模块经压缩和加密后被封装在主模块mssecmgr.ocx或其他模块的资源节中，只有在病毒启动时，才被解密解压缩，并被注入到可信系统进程空间中。在代码注入的过程中，功能模块基本上不以文件的形式写到文件系统中；功能模块注入时不采用CreateRomteThread（）和LoadLibrary（）等易被安全软件监控的函数，而是通过PE文件内存动态加载技术实现程序模块远程注入。 2.4SQLite3文件型数据库SQLite3是一个轻量级的文件型数据库系统，它占用资源非常低，其库模块文件只有几百KB大小，能够支持Windows、Linux、Unix等主流操作系统，同时能够和很多程序语言相结合，比如Tcl、PHP、Java等，还有ODBC接口，其处理效率也很高。在火焰病毒中集成了一个SQLite3库，通过该数据库系统可以对窃取的各类信息进行高效存储管理。 2.5Lua脚本语言Lua是一个小巧的脚本语言，其设计目的是嵌入应用程序，为应用程序提供灵活的扩展和定制功能，应用程序使用Lua作为自己的嵌入式脚本语言，以此来实现可配置和可扩展。火焰病毒中的部分程序模块采用Lua脚本实现，根据分析，判断火焰病毒中有3000多行的Lua脚本代码量。 2.6指挥控制服务器和蓝牙通信火焰病毒根据配置文件中设置的指挥控制（C&amp;C）服务器的域名进行连接，当发现可以连接指挥控制服务器后，就会将获取的信息传送到指挥控制服务器上，并且从该服务器上获取相关的指令和配置。同时，火焰病毒在隔离网络间进行数据传送时还巧妙地利用了蓝牙通信技术――当检测到被感染的主机有蓝牙设备时，该病毒将打开蓝牙设备，检测周围有哪些可以连接的蓝牙设备，将检测到的信息记录在信息文件中；同时，该病毒还可以将被感染的主机设置为“beacon”（发送）模式，其他感染的主机通过蓝牙通信进行连接并传送数据。这样，两台主机即使在两个隔离的网络之间仍然可以通过蓝牙设备进行数据传输。 三、火焰病毒的特点3.1模块化程度高火焰病毒中包含几十个模块，多数模块是可以配置或上传的。获取的火焰病毒主模块样本最大的有6MB，最小的只有900KB――最小的病毒样本开始执行后将通过网络下载所需的其他模块。另外，模块化程度高也便于病毒的升级更新。在该病毒潜伏的这些年中，虽然网络技术和Windows操作系统版本经过了几代的发展，但是由于病毒的开发者不断地研发出新的模块，上传到火焰病毒中，使该病毒技术领先、机制复杂、威胁程度高。 3.2信息获取能力强火焰病毒根据配置，可以通过键盘记录、屏幕截取、麦克风录音、网络共享监测、存储介质遍历、移动介质监测、网络嗅探、无线网络（Wi-Fi）探测、蓝牙连接监视等手段获取各种敏感和机密信息，具有极强的信息获取能力。 3.3渗透扩散能力强火焰病毒可以通过USB移动存储介质的autorun.inf、针对USB移动存储介质的MS10-046漏洞、针对Windows系统的MS10-061远程漏洞和补丁升级中间人攻击、SqlServer的远程工作任务、共享目录和域管理员权限等多种方法对局域网或隔离网络中其他主机进行渗透扩散。这些方法涉及移动介质、操作系统、数据库和域等多个层面。 3.4可控性强火焰病毒之所以能够潜伏这么长时间而不被发现，主要是由于其具有极强的可控性。当该病毒检测到可以连接互联网时，就将获取到的各种信息通过指挥控制（C&amp;C）服务器发送给病毒的操纵者；病毒的操纵者根据在目标网络中获取的信息，通过指挥控制服务器向火焰病毒传送包括加载功能模块、渗透扩展方式、信息获取方式、渗透扩展次数、自毁模块等配置，对火焰病毒的行为进行控制，这样可以在获取各种信息的同时最大程度地隐藏其行为，防止被发现。 3.5自身防护能力强火焰病毒中包含防护软件探测模块，该模块可以对被感染主机安装的防护软件进行探测，能够根据主机的安全状况决定是否执行、如何生成附属文件的名称或者是否将火焰病毒从被感染主机上彻底清除。另外，火焰病毒采用了模块的“隐蔽”注入，使得其更加难以被发现。 3.6数据机密性高火焰病毒采用了三种数据压缩算法、五种加密算法和五种文件格式对病毒文件进行压缩加密处理，使得火焰病毒附属文件的机密性很高，通过常规的文件内容分析和扫描根本无法检测出与火焰病毒相关的敏感信息。 发展及防护2019年中国互联网网络安全报告中说道：“2019年，在我国相关部门持续开展的网络安全威胁治理下，分布式拒绝服务攻击（以下简称DDoS攻击）、高级持续性威胁攻击（以下简称APT攻击）、漏洞威胁、数据安全隐患、移动互联网恶意程序、网络黑灰色产业链（以下简称黑灰产）、工业控制系统安全威胁总体下降，但呈现出许多新的特点，带来新的风险与挑战。”网络安全，不能有一丝懈怠。 与以往炮火纷飞的传统作战方式不同，网络战是一种隐蔽无声的全新作战方式，它不仅活跃在战争和各类冲突中，而且闪烁于平时的各种政治、经济、军事、文化和科技等活动中。不论是哪个国家和民族，要想立足未来，必须占领互联网这个制高点。 2017年4月13日，永恒之蓝漏洞被曝光，为大众所熟知，在该漏洞曝光后的一个月，5月12日，勒索病毒WannaCry利用永恒之蓝漏洞肆虐全球，以类似于蠕虫病毒的方式传播，攻击主机并加密主机上存储的文件，然后要求以比特币的形式支付赎金，据报道包括美国、英国、中国、俄罗斯、西班牙、意大利、越南等百余个国家均遭受大规模攻击。我国的许多行业机构和大型企业也被攻击，有的单位甚至“全军覆没”，损失之严重为近年来所罕见。至少150个国家、30万名用户中招，造成超80亿美元的损失。据火绒实验室技术分析追溯发现，该病毒分蠕虫部分及勒索病毒部分，前者用于传播和释放病毒，后者攻击用户加密文件。 火焰病毒具有蠕虫的特征，其主要特性是捕获数据和窃取信息，通过劫持Windows系统的升级程序，把自己安装在目标计算机上。升级程序本来自带数字签名和证书，以防止程序被篡改，但火焰病毒的制造者伪造了这个证书，说明其针对hash算法进行了碰撞攻击。 MD5是一种被广泛使用的hash函数。虽然王小云教授已经证明MD5是不安全的，但在我的记忆里，MD5碰撞攻击是非常复杂并且耗费大量时间和资源的。在看到火焰病毒竟然可以轻易仿造数字签名证书，我感到非常震撼。 通过了解火焰病毒的攻击原理，我觉得可以通过以下方式进行防护： 安装安全防护软件，及时更新病毒数据库； 进一步加强企业内网安全建设，关闭主机中不必要的网络服务端口； 加强对口令以及移动存储设备的安全管理。 此外，由于火焰病毒可以不知不觉的控制蓝牙，麦克风，摄像头等硬件设备，那我们要格外关注相对应的设备开关标志，比如大部分电脑的摄像头被使用时，旁边的小灯会亮起；麦克风被使用时，任务栏会有小话筒标志。 大数据背景下,计算机网络技术深刻影响着大众的生活、生产。人类的吃穿住行玩乐已经离不开互联网和信息，在信息化社会中，信息才是主宰，因此保护信息安全至关重要。现在中国相对稳定的安全环境中存在着不安全因素，“安而不忘危，存而不忘亡，治而不忘乱。”居安思危，思则有备，备则无患，在任何时刻都不能放松警惕。","categories":[{"name":"恶意代码防范","slug":"恶意代码防范","permalink":"https://www.zlgan.top/categories/%E6%81%B6%E6%84%8F%E4%BB%A3%E7%A0%81%E9%98%B2%E8%8C%83/"}],"tags":[{"name":"恶意代码防范","slug":"恶意代码防范","permalink":"https://www.zlgan.top/tags/%E6%81%B6%E6%84%8F%E4%BB%A3%E7%A0%81%E9%98%B2%E8%8C%83/"},{"name":"病毒","slug":"病毒","permalink":"https://www.zlgan.top/tags/%E7%97%85%E6%AF%92/"},{"name":"火焰","slug":"火焰","permalink":"https://www.zlgan.top/tags/%E7%81%AB%E7%84%B0/"},{"name":"Flame","slug":"Flame","permalink":"https://www.zlgan.top/tags/Flame/"}],"author":"gzl"},{"title":"等级保护&分级保护的联系与区别","slug":"等级保护&分级保护的联系与区别","date":"2022-03-25T08:10:33.000Z","updated":"2022-03-28T08:38:33.474Z","comments":true,"path":"2022/03/25/等级保护&分级保护的联系与区别/","link":"","permalink":"https://www.zlgan.top/2022/03/25/%E7%AD%89%E7%BA%A7%E4%BF%9D%E6%8A%A4&%E5%88%86%E7%BA%A7%E4%BF%9D%E6%8A%A4%E7%9A%84%E8%81%94%E7%B3%BB%E4%B8%8E%E5%8C%BA%E5%88%AB/","excerpt":"","text":"等级保护&amp;分级保护的联系与区别一、等级保护&amp;分级保护的不同定义等级保护信息安全等级保护是指对国家秘密信息、法人和其他组织及公民的专有信息以及公开信息和储存、传输、处理这些信息的信息系统分等级实行安全保护，对信息系统中使用的信息安全产品实行按等级管理，对信息系统中发生的信息安全事件分等级响应、处置。 分级保护涉密信息系统分级保护是指涉密信息系统的建设使用单位根据分级保护管理办法和有关标准，对涉密信息系统分等级实施保护，各级保密工作部门根据涉密信息系统的保护等级实施监督管理，确保系统和信息安全。 二、等级保护与分级保护不同的适用对象 （本质区别）等级保护国家安全信息等级保护是实施信息安全管理的一项法定制度，重点保护的对象是非涉密的涉及国计民生的重要信息系统和通信基础信息系统。 分级保护涉密信息系统分级保护是国家信息安全等级保护的重要组成部分，是等级保护在涉密领域的具体体现。 三、等级保护和分级保护不同的发起部门和主管部门等级保护等级保护由公安部门发起，其主管单位及相应管理职责如下所示： 公安机关：等级保护工作的主管部门，负责信息安全等级保护工作的监督、检查、指导； 国家保密工作部门：负责等级保护工作中有关保密工作的监督、检查、指导； 国家密码管理部门：负责等级保护工作中有关密码工作的监督、检查、指导； 国务院信息办（国信办）及地方信息化领导小组办事机构：负责等级保护工作部门间的协调，涉及国家秘密信息系统的等级保护监督管理工作由国家保密工作部门负责。 分级保护分级保护由国家保密局发起，其主管单位及相应管理职责如下所示： 国家保密局：监督，检查，指导； 地方各级保密局：监督，检查，指导； 中央和国家机关(本部门)：主管和指导； 建设使用单位：具体实施。 四、等级保护和分级保护不同的政策依据等级保护政策依据《中华人民共和国计算机信息系统安全保护条例》（国务院147号令，1994年）； 《国家信息化领导小组关于加强信息安全保障工作的意见》（中办发[2003]27号）； 《关于信息安全等级保护工作的实施意见》（公通字[2004]66号）； 《信息安全等级保护管理办法》（公通字[2007]43号）； 《关于开展全国重要信息系统安全等级保护定级工作的通知》（公信安[2007]861号）； 《关于加强国家电子政务工程建设项目信息安全风险评估工作的通知》(发改高技[2008]2071号)。 分级保护政策依据《关于加强信息安全保障工作中保密管理的若干意见》（中保委发[2004]7号）； 《涉及国家秘密的信息系统分级保护管理办法》（国保发[2005]16号）。 五、标准体系等级保护国家标准（GB、GB&#x2F;T） 分级保护国家保密标准（BMB,强制执行） 六、系统定级等级保护信息系统的安全保护等级应当根据信息系统在国家安全、经济建设、社会生活中的重要程度，信息系统遭到破坏后对国家安全、社会秩序、公共利益以及公民、法人和其他组织的合法权益的危害程度等因素确定，由低到高划分为五个等级：第一级（自主保护）、第二级（指导保护）、第三级（监督保护）、第四级（强制保护）、第五级（专控保护）。 分级保护涉密信息系统按照所处理信息的最高密级，由低到高划分为秘密、机密和绝密三个等级 之间的联系（对应关系）涉密信息系统建设使用单位应当依据涉密信息系统分级保护管理规范和技术标准，按照秘密、机密、绝密三级的不同要求, 结合系统实际进行方案设计，实施分级保护，其保护水平总体上不低于国家信息安全等级保护第三级、第四级、第五级的水平。 秘密级对应三级、机密级对应四级、绝密级对应五级。 七、工作内容等级保护工作包括系统定级、系统备案、安全建设整改、等级测评和监督检查五个环节。 分级保护工作包括系统定级、方案设计、工程实施、系统测评、系统审批、日常管理、测评与检查、系统废止八个环节。 八、测评频率等级保护各级别测评频率： 第二级信息系统：应每两年至少进行一次等级测评； 第三级信息系统：应每年至少进行一次等级测评； 第四级信息系统：应每半年至少进行一次等级测评。 第一级信息系统不需测评。 第五级信息系统一般适用于国家重要领域、重要部门中的极端重要系统，特殊行业特殊要求，不在等保测评机构的测评范畴。应当根据特殊安全要求进行等级测评。 分级保护各级别测评频率： 秘密级、机密级信息系统：应每两年至少进行一次安全保密测评或保密检查； 绝密级信息系统：应每年至少进行一次安全保密测评或保密检查。 九、测评机构资质要求等级保护测评机构资质由国家信息安全等级保护工作协调小组办公室授予。 分级保护测评机构资质由国家保密工作部门授予。","categories":[{"name":"保密技术概论","slug":"保密技术概论","permalink":"https://www.zlgan.top/categories/%E4%BF%9D%E5%AF%86%E6%8A%80%E6%9C%AF%E6%A6%82%E8%AE%BA/"}],"tags":[{"name":"保密","slug":"保密","permalink":"https://www.zlgan.top/tags/%E4%BF%9D%E5%AF%86/"},{"name":"等级保护","slug":"等级保护","permalink":"https://www.zlgan.top/tags/%E7%AD%89%E7%BA%A7%E4%BF%9D%E6%8A%A4/"},{"name":"分级保护","slug":"分级保护","permalink":"https://www.zlgan.top/tags/%E5%88%86%E7%BA%A7%E4%BF%9D%E6%8A%A4/"}],"author":"gzl"}],"categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://www.zlgan.top/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"保密技术概论","slug":"保密技术概论","permalink":"https://www.zlgan.top/categories/%E4%BF%9D%E5%AF%86%E6%8A%80%E6%9C%AF%E6%A6%82%E8%AE%BA/"},{"name":"入侵检测","slug":"入侵检测","permalink":"https://www.zlgan.top/categories/%E5%85%A5%E4%BE%B5%E6%A3%80%E6%B5%8B/"},{"name":"恶意代码防范","slug":"恶意代码防范","permalink":"https://www.zlgan.top/categories/%E6%81%B6%E6%84%8F%E4%BB%A3%E7%A0%81%E9%98%B2%E8%8C%83/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://www.zlgan.top/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"终生学习","slug":"终生学习","permalink":"https://www.zlgan.top/tags/%E7%BB%88%E7%94%9F%E5%AD%A6%E4%B9%A0/"},{"name":"BM资质","slug":"BM资质","permalink":"https://www.zlgan.top/tags/BM%E8%B5%84%E8%B4%A8/"},{"name":"SM集成资质","slug":"SM集成资质","permalink":"https://www.zlgan.top/tags/SM%E9%9B%86%E6%88%90%E8%B5%84%E8%B4%A8/"},{"name":"保密","slug":"保密","permalink":"https://www.zlgan.top/tags/%E4%BF%9D%E5%AF%86/"},{"name":"Anomaly Detection","slug":"Anomaly-Detection","permalink":"https://www.zlgan.top/tags/Anomaly-Detection/"},{"name":"ANN","slug":"ANN","permalink":"https://www.zlgan.top/tags/ANN/"},{"name":"CNN","slug":"CNN","permalink":"https://www.zlgan.top/tags/CNN/"},{"name":"KNN","slug":"KNN","permalink":"https://www.zlgan.top/tags/KNN/"},{"name":"恶意代码防范","slug":"恶意代码防范","permalink":"https://www.zlgan.top/tags/%E6%81%B6%E6%84%8F%E4%BB%A3%E7%A0%81%E9%98%B2%E8%8C%83/"},{"name":"病毒","slug":"病毒","permalink":"https://www.zlgan.top/tags/%E7%97%85%E6%AF%92/"},{"name":"火焰","slug":"火焰","permalink":"https://www.zlgan.top/tags/%E7%81%AB%E7%84%B0/"},{"name":"Flame","slug":"Flame","permalink":"https://www.zlgan.top/tags/Flame/"},{"name":"等级保护","slug":"等级保护","permalink":"https://www.zlgan.top/tags/%E7%AD%89%E7%BA%A7%E4%BF%9D%E6%8A%A4/"},{"name":"分级保护","slug":"分级保护","permalink":"https://www.zlgan.top/tags/%E5%88%86%E7%BA%A7%E4%BF%9D%E6%8A%A4/"}]}